{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "L5gaYTNx_Byl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogdlw0zfFbr0",
        "outputId": "10838f40-9f67-4d96-e720-bb8578f22879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "WCNesPq6Oe-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "fever=pd.read_csv(\"/content/drive/MyDrive/My Work/IR/FEVER.csv\")\n",
        "characters = ['[',\"'\",',',']']\n",
        "fever['claim'] = fever['claim'].tolist()\n",
        "for i in range(len(fever['claim'])):\n",
        "  fever['claim'][i] = [ word for word in fever['claim'][i] if word not in characters]\n",
        "model1 = gensim.models.Word2Vec(list(fever['claim']),min_count=1)\n"
      ],
      "metadata": {
        "id": "E4wm0dJGU7rT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "886f0a93-f63b-4d59-9a0d-979b796a9bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fever['claim']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXgaBShIpRIz",
        "outputId": "34be222d-4f39-4ada-dd4c-e6a290f406f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         [n, i, k, o, l, a, j,  , w, o, r, k, e, d,  , ...\n",
              "1         [r, o, m, a, n,  , a, t, w, o, o, d,  , c, o, ...\n",
              "2         [h, i, s, t, o, r, y,  , a, r, t,  , i, n, c, ...\n",
              "3         [a, d, r, i, e, n, n, e,  , b, a, i, l, o, n, ...\n",
              "4         [h, o, m, e, l, a, n, d,  , a, m, e, r, i, c, ...\n",
              "                                ...                        \n",
              "109805    [l, e, d,  , z, e, p, p, e, l, i, n,  , r, e, ...\n",
              "109806              [t, a, a, l,  , r, o, m, a, n, t, i, c]\n",
              "109807    [h, e, r,  , s, t, a, r, s,  , a, m, e, r, i, ...\n",
              "109808    [t, o, l, k, i, e, n,  , c, r, e, a, t, e, d, ...\n",
              "109809    [s, u, s, a, n,  , s, a, r, a, n, d, o, n,  , ...\n",
              "Name: claim, Length: 109810, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "list1 = list('roman')+list('atwood')\n",
        "list2 = list('roman')+ list('atwood')+list('content')+list('creator')\n",
        "print(list2)\n",
        "v1 = model1.wv[list1]\n",
        "v2 = model1.wv[list2]\n",
        "cossim=torch.nn.CosineSimilarity(dim=0, eps=1e-08)\n",
        "v1 = torch.norm(torch.from_numpy(v1),dim=0)\n",
        "v2 = torch.norm(torch.from_numpy(v2),dim=0)\n",
        "cossim(v1,v2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noiOQFD4Ykyp",
        "outputId": "2b033c57-5ead-46ea-bef1-a4e6e3b392be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['r', 'o', 'm', 'a', 'n', 'a', 't', 'w', 'o', 'o', 'd', 'c', 'o', 'n', 't', 'e', 'n', 't', 'c', 'r', 'e', 'a', 't', 'o', 'r']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9911)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "tweets = []\n",
        "for line in open(\"/content/drive/MyDrive/My Work/IR/train.jsonl\", 'r'):\n",
        "    tweets.append(json.loads(line))\n",
        "evidence = []\n",
        "for t in tweets:\n",
        "  evi = []\n",
        "  if(t['label']==\"NOT ENOUGH INFO\"):\n",
        "    continue\n",
        "  if len(t['evidence'][0]) >1:\n",
        "    evi.append(t['evidence'][0])\n",
        "    evi.append(len(t['evidence'][0]))\n",
        "  else :\n",
        "    evi.append(t['evidence'][0][0])\n",
        "    if t['evidence'][0][0][2] is None:\n",
        "      evi.append(None)\n",
        "    else: \n",
        "      evi.append(len(t['evidence'][0]))\n",
        "  evidence.append(evi)\n",
        "print(evidence[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECUpSftelA1S",
        "outputId": "5e583867-76bc-40e7-8692-fc3f96fcdfaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[92206, 104971, 'Nikolaj_Coster-Waldau', 7], [92206, 104971, 'Fox_Broadcasting_Company', 0]], 2], [[174271, 187498, 'Roman_Atwood', 1], 1], [[255136, 254645, 'History_of_art', 2], 1], [[180804, 193183, 'Adrienne_Bailon', 0], 1], [[[151831, 166598, 'Homeland_-LRB-TV_series-RRB-', 0], [151831, 166598, 'Prisoners_of_War_-LRB-TV_series-RRB-', 0]], 2], [[49158, 58489, 'Boston_Celtics', 3], 1], [[23513, 28977, 'The_Ten_Commandments_-LRB-1956_film-RRB-', 0], 1], [[269479, 265800, 'Tetris', 18], 1], [[56492, 66697, 'Cyndi_Lauper', 2], 1], [[93100, 106004, 'The_Hunger_Games_-LRB-film-RRB-', 0], 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual = []\n",
        "for t in tweets:\n",
        "  if(t['label']==\"NOT ENOUGH INFO\"):\n",
        "    continue\n",
        "  evi = []\n",
        "  for i in range(len(t['evidence'])):\n",
        "    t['evidence'][i][0][3] = i+1\n",
        "    evi.append(t['evidence'][i][0])\n",
        "  actual.append([evi])"
      ],
      "metadata": {
        "id": "QOt_Zr5YYSKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim_list = [''.join(c) for c in fever['claim']]"
      ],
      "metadata": {
        "id": "gy9oBFNVDnQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_cossim(query):\n",
        "  max = 0\n",
        "  query_list = []\n",
        "  doc_retrieve_list = []\n",
        "  actual = []\n",
        "  for word in query.lower():\n",
        "    query_list = query_list+list(word)\n",
        "  characters = ['[',\"'\",',',']','0','1','2','3','4','5','6','7','8','9','-','.']\n",
        "  cossim=torch.nn.CosineSimilarity(dim=0, eps=1e-08)\n",
        "  query_list = [ word for word in query_list if word not in characters]\n",
        "  v2 = model1.wv[query_list]\n",
        "  v2 = torch.norm(torch.from_numpy(v2),dim=0)\n",
        "  for i in range(len(fever['claim'])):\n",
        "    try:\n",
        "      v1 = model1.wv[fever['claim'][i]]\n",
        "      v1 = torch.norm(torch.from_numpy(v1),dim=0)\n",
        "      if max< cossim(v1,v2):\n",
        "        max = cossim(v1,v2)\n",
        "        if type(evidence[i][0][0]) == list:\n",
        "          e = []\n",
        "          for evi in fever['evidence'][i][0]:\n",
        "            e.append(evi[2])\n",
        "        else:\n",
        "          e = evidence[i][0][2]\n",
        "        doc_retrieve_list.append(e) \n",
        "    except:\n",
        "      pass\n",
        "  doc_retrieve_list = doc_retrieve_list[::-1]\n",
        "  doc_retrieve_list = [i for n, i in enumerate(doc_retrieve_list) if i not in doc_retrieve_list[:n]]\n",
        "  if len(doc_retrieve_list) > 5:\n",
        "    doc_retrieve_list = doc_retrieve_list[:5]\n",
        "  return doc_retrieve_list\n",
        "\n",
        "evi = []\n",
        "predicted = []\n",
        "for i in range(10):\n",
        "  query = claim_list[i]\n",
        "  \n",
        "  r = calc_cossim(query)\n",
        "  if r is not None:\n",
        "    pred = []\n",
        "    for i in range(len(r)):\n",
        "      pred.append([r[i],i+1])\n",
        "    predicted.append(pred)\n",
        "predicted = list(filter(lambda a: a != [], predicted))\n",
        "print(predicted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DicAkO9je2cE",
        "outputId": "0c45ae99-838c-4079-d58e-ee6960065b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['Roman_Atwood', 1]], [['History_of_art', 1]], [['Adrienne_Bailon', 1], ['History_of_art', 2]], [['History_of_art', 1]], [['Boston_Celtics', 1], ['History_of_art', 2]], [['The_Ten_Commandments_-LRB-1956_film-RRB-', 1], ['Boston_Celtics', 2], ['History_of_art', 3]], [['Tetris', 1], ['History_of_art', 2]], [['Cyndi_Lauper', 1]], [['The_Hunger_Games_-LRB-film-RRB-', 1], ['Boston_Celtics', 2], ['History_of_art', 3]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual = actual[1:]"
      ],
      "metadata": {
        "id": "wMYMS0_henp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = input('Enter query\\n')\n",
        "print(calc_cossim(query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0BiDeZPIYbF",
        "outputId": "3cd3730d-8d72-4c08-bf86-d9528d0676bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter query\n",
            "nikolaj\n",
            "['Nikolaj_Coster-Waldau', 'Nicole_Kidman', 'Nicki_Minaj', 'International_relations', 'Jennifer_Aniston']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jaXvGWZXLHDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F_nyOUw7x0E"
      },
      "outputs": [],
      "source": [
        "import six\n",
        "\n",
        "def check_predicted_evidence_format(instance):\n",
        "    if 'predicted_evidence' in instance.keys() and len(instance['predicted_evidence']):\n",
        "        assert all(isinstance(prediction, list)\n",
        "                   for prediction in instance[\"predicted_evidence\"]), \\\n",
        "            \"Predicted evidence must be a list of (page,line) lists\"\n",
        "\n",
        "        assert all(len(prediction) == 2\n",
        "                   for prediction in instance[\"predicted_evidence\"]), \\\n",
        "            \"Predicted evidence must be a list of (page,line) lists\"\n",
        "\n",
        "        assert all(isinstance(prediction[0], six.string_types)\n",
        "                    for prediction in instance[\"predicted_evidence\"]), \\\n",
        "            \"Predicted evidence must be a list of (page<string>,line<int>) lists\"\n",
        "\n",
        "        assert all(isinstance(prediction[1], int)\n",
        "                   for prediction in instance[\"predicted_evidence\"]), \\\n",
        "            \"Predicted evidence must be a list of (page<string>,line<int>) lists\"\n",
        "\n",
        "\n",
        "def is_correct_label(instance):\n",
        "    return instance[\"label\"].upper() == instance[\"predicted_label\"].upper()\n",
        "\n",
        "\n",
        "def is_strictly_correct(instance, max_evidence=None):\n",
        "    #Strict evidence matching is only for NEI class\n",
        "    check_predicted_evidence_format(instance)\n",
        "\n",
        "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\" and is_correct_label(instance):\n",
        "        assert 'predicted_evidence' in instance, \"Predicted evidence must be provided for strict scoring\"\n",
        "\n",
        "        if max_evidence is None:\n",
        "            max_evidence = len(instance[\"predicted_evidence\"])\n",
        "\n",
        "\n",
        "        for evience_group in instance[\"evidence\"]:\n",
        "            #Filter out the annotation ids. We just want the evidence page and line number\n",
        "            actual_sentences = [[e[2], e[3]] for e in evience_group]\n",
        "            #Only return true if an entire group of actual sentences is in the predicted sentences\n",
        "            if all([actual_sent in instance[\"predicted_evidence\"][:max_evidence] for actual_sent in actual_sentences]):\n",
        "                return True\n",
        "\n",
        "    #If the class is NEI, we don't score the evidence retrieval component\n",
        "    elif instance[\"label\"].upper() == \"NOT ENOUGH INFO\" and is_correct_label(instance):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def evidence_macro_precision(instance, max_evidence=None):\n",
        "    this_precision = 0.0\n",
        "    this_precision_hits = 0.0\n",
        "\n",
        "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
        "        all_evi = [[e[2], e[3]] for eg in instance[\"evidence\"] for e in eg if e[3] is not None]\n",
        "\n",
        "        predicted_evidence = instance[\"predicted_evidence\"] if max_evidence is None else \\\n",
        "                                                                        instance[\"predicted_evidence\"][:max_evidence]\n",
        "\n",
        "        for prediction in predicted_evidence:\n",
        "            if prediction in all_evi:\n",
        "                this_precision += 1.0\n",
        "            this_precision_hits += 1.0\n",
        "\n",
        "        return (this_precision / this_precision_hits) if this_precision_hits > 0 else 1.0, 1.0\n",
        "\n",
        "    return 0.0, 0.0\n",
        "\n",
        "def evidence_macro_recall(instance, max_evidence=None):\n",
        "    # We only want to score F1/Precision/Recall of recalled evidence for NEI claims\n",
        "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
        "        # If there's no evidence to predict, return 1\n",
        "        if len(instance[\"evidence\"]) == 0 or all([len(eg) == 0 for eg in instance]):\n",
        "           return 1.0, 1.0\n",
        "\n",
        "        predicted_evidence = instance[\"predicted_evidence\"] if max_evidence is None else \\\n",
        "                                                                        instance[\"predicted_evidence\"][:max_evidence]\n",
        "\n",
        "        for evidence_group in instance[\"evidence\"]:\n",
        "            evidence = [[e[2], e[3]] for e in evidence_group]\n",
        "            if all([item in predicted_evidence for item in evidence]):\n",
        "                # We only want to score complete groups of evidence. Incomplete groups are worthless.\n",
        "                return 1.0, 1.0\n",
        "        return 0.0, 1.0\n",
        "    return 0.0, 0.0\n",
        "\n",
        "\n",
        "# Micro is not used. This code is just included to demostrate our model of macro/micro\n",
        "def evidence_micro_precision(instance):\n",
        "    this_precision = 0\n",
        "    this_precision_hits = 0\n",
        "\n",
        "    # We only want to score Macro F1/Precision/Recall of recalled evidence for NEI claims\n",
        "    if instance[\"label\"].upper() != \"NOT ENOUGH INFO\":\n",
        "        all_evi = [[e[2], e[3]] for eg in instance[\"evidence\"] for e in eg if e[3] is not None]\n",
        "\n",
        "        for prediction in instance[\"predicted_evidence\"]:\n",
        "            if prediction in all_evi:\n",
        "                this_precision += 1.0\n",
        "            this_precision_hits += 1.0\n",
        "\n",
        "    return this_precision, this_precision_hits\n",
        "\n",
        "\n",
        "def fever_score(predictions,actual=None, max_evidence=5):\n",
        "    correct = 0\n",
        "    strict = 0\n",
        "\n",
        "    macro_precision = 0\n",
        "    macro_precision_hits = 0\n",
        "\n",
        "    macro_recall = 0\n",
        "    macro_recall_hits = 0\n",
        "\n",
        "    for idx,instance in enumerate(predictions):\n",
        "        assert 'predicted_evidence' in instance.keys(), 'evidence must be provided for the prediction'\n",
        "\n",
        "        #If it's a blind test set, we need to copy in the values from the actual data\n",
        "        if 'evidence' not in instance or 'label' not in instance:\n",
        "            assert actual is not None, 'in blind evaluation mode, actual data must be provided'\n",
        "            assert len(actual) == len(predictions), 'actual data and predicted data length must match'\n",
        "            assert 'evidence' in actual[idx].keys(), 'evidence must be provided for the actual evidence'\n",
        "            instance['evidence'] = actual[idx]['evidence']\n",
        "            instance['label'] = actual[idx]['label']\n",
        "\n",
        "        assert 'evidence' in instance.keys(), 'gold evidence must be provided'\n",
        "\n",
        "        if is_correct_label(instance):\n",
        "            correct += 1.0\n",
        "\n",
        "            if is_strictly_correct(instance, max_evidence):\n",
        "                strict+=1.0\n",
        "\n",
        "        macro_prec = evidence_macro_precision(instance, max_evidence)\n",
        "        macro_precision += macro_prec[0]\n",
        "        macro_precision_hits += macro_prec[1]\n",
        "\n",
        "        macro_rec = evidence_macro_recall(instance, max_evidence)\n",
        "        macro_recall += macro_rec[0]\n",
        "        macro_recall_hits += macro_rec[1]\n",
        "\n",
        "    total = len(predictions)\n",
        "\n",
        "    strict_score = strict / total\n",
        "    acc_score = correct / total\n",
        "\n",
        "    pr = (macro_precision / macro_precision_hits) if macro_precision_hits > 0 else 1.0\n",
        "    rec = (macro_recall / macro_recall_hits) if macro_recall_hits > 0 else 0.0\n",
        "    try :\n",
        "      f1 = 2.0 * pr * rec / (pr + rec)\n",
        "    except:\n",
        "      f1 = 0\n",
        "\n",
        "    return strict_score, acc_score, pr, rec, f1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = \"SUPPORTS\"\n",
        "scores = []\n",
        "for i in range(len(predicted)):\n",
        "  instance = {\n",
        "      \"label\":label, \n",
        "      \"predicted_label\":label, \n",
        "      \"predicted_evidence\":predicted[i], \n",
        "      \"evidence\": actual[i]\n",
        "      }\n",
        "  strict_score, label_accuracy, precision, recall, f1 = fever_score([instance])\n",
        "  scores.append([strict_score, label_accuracy, precision, recall, f1])\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lHP9zrpdy2s",
        "outputId": "1be833dd-326e-4c2a-b50b-69ddba55aa85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0, 1.0, 0.0, 0.0, 0], [0.0, 1.0, 0.0, 0.0, 0], [0.0, 1.0, 0.0, 0.0, 0], [0.0, 1.0, 0.0, 0.0, 0], [0.0, 1.0, 0.0, 0.0, 0], [0.0, 1.0, 0.0, 0.0, 0], [0.0, 1.0, 0.0, 0.0, 0], [0.0, 1.0, 0.0, 0.0, 0], [0.0, 1.0, 0.0, 0.0, 0]]\n"
          ]
        }
      ]
    }
  ]
}