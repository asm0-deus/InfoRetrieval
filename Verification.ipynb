{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.   SUPPORTS\n",
      "Roman Atwood is a content creator.   SUPPORTS\n"
     ]
    }
   ],
   "source": [
    "filetrain=pd.read_csv(\"claimsss.csv\")\n",
    "col1=filetrain['claim']\n",
    "col2=filetrain['label']\n",
    "for i in range(2):\n",
    "    print(col1[i],\" \",col2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "text1=[]\n",
    "for i in range(len(col1)):\n",
    "    soup = BeautifulSoup(col1[i])\n",
    "    text = soup.get_text()\n",
    "    text1.append(text)         #Removed tags and urls\n",
    "    \n",
    "# for i in range(2):\n",
    "#     print(text1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') # For tokenizers\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "sum=0\n",
    "stop_words=set(stopwords.words('english'))\n",
    "for i in range(len(text1)):\n",
    "    words.append(nltk.word_tokenize(text1[i]))\n",
    "    words[i]=[word.lower() for word in words[i] if word.isalnum()]   #removed non alphanumeric\n",
    "    words[i]=[word.lower() for word in words[i] if word not in stop_words]  #removed stopwords\n",
    "    sum=sum+len(words[i])\n",
    "    # print(len(words[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum2=0\n",
    "words2=words\n",
    "words3=[]\n",
    "Max=0\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(words2)):\n",
    "    tempstr=\"\"\n",
    "    for wd in range(len(words2[i])):\n",
    "        words2[i][wd]=lemmatizer.lemmatize(words2[i][wd]) \n",
    "        tempstr=tempstr+' '+words2[i][wd]\n",
    "    words3.append(tempstr)\n",
    "    # words2[i]=set(words2[i])\n",
    "    sum2=sum2+(len(words2[i]))       #lemmatization completed\n",
    "    if(len(words2[i])>Max):\n",
    "        Max=len(words2[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews =  80035 \n",
      "Negative reviews =  29775\n",
      "average no. of words per review =  5.175922047172389\n",
      "maximum number of words =  56\n"
     ]
    }
   ],
   "source": [
    "# Print Statistics of Data like avg length of sentence , proposition of data w.r.t class labels\n",
    "# print(len(col1))\n",
    "count=0\n",
    "cnt=0\n",
    "for i in col2:\n",
    "    if(i=='SUPPORTS'):\n",
    "        count=count+1\n",
    "    if(i=='REFUTES'):\n",
    "        cnt=cnt+1\n",
    "print(\"Positive reviews = \",count,\"\\nNegative reviews = \",cnt)\n",
    "print(\"average no. of words per review = \",sum/len(col2))\n",
    "print(\"maximum number of words = \",Max)\n",
    "# print(\"average no. of words after lemmatization = \",sum2/len(col2))\n",
    "# print(col2.count(\"positive\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "vocab_size = 3000 # choose based on statistics\n",
    "oov_tok = '<OOK>'\n",
    "embedding_dim = 100\n",
    "max_length = 200# choose based on statistics, for example 150 to 200\n",
    "padding_type='post'\n",
    "trunc_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = words3\n",
    "labels = col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n",
      "['REFUTES' 'SUPPORTS']\n"
     ]
    }
   ],
   "source": [
    "# Use label encoder to encode labels. Convert to 0/1\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)\n",
    "print((encoded_labels))\n",
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, test_sentences, train_labels, test_labels=train_test_split(reviews,encoded_labels,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 200, 100)          300000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                3096      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 387,601\n",
      "Trainable params: 387,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(24, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2471/2471 [==============================] - 277s 111ms/step - loss: 0.5096 - accuracy: 0.7728 - val_loss: 0.4913 - val_accuracy: 0.7787\n",
      "Epoch 2/5\n",
      "2471/2471 [==============================] - 414s 168ms/step - loss: 0.4707 - accuracy: 0.7878 - val_loss: 0.4833 - val_accuracy: 0.7809\n",
      "Epoch 3/5\n",
      "2471/2471 [==============================] - 420s 170ms/step - loss: 0.4486 - accuracy: 0.7957 - val_loss: 0.4830 - val_accuracy: 0.7836\n",
      "Epoch 4/5\n",
      "2471/2471 [==============================] - 336s 136ms/step - loss: 0.4309 - accuracy: 0.8005 - val_loss: 0.4877 - val_accuracy: 0.7834\n",
      "Epoch 5/5\n",
      "2471/2471 [==============================] - 365s 148ms/step - loss: 0.4154 - accuracy: 0.8067 - val_loss: 0.5022 - val_accuracy: 0.7779\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "history = model.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=1, \n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.32      0.44      5969\n",
      "    positive       0.79      0.95      0.86     15993\n",
      "\n",
      "    accuracy                           0.78     21962\n",
      "   macro avg       0.75      0.64      0.65     21962\n",
      "weighted avg       0.77      0.78      0.75     21962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on Test data\n",
    "\n",
    "prediction = model.predict(test_padded)\n",
    "\n",
    "\n",
    "# Get probabilities\n",
    "prob=[]\n",
    "for i in range(len(prediction)):\n",
    "    val=prediction[i][0]\n",
    "    prob.append(val)\n",
    "\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "lstmlabels=[]\n",
    "for i in range(len(prob)):\n",
    "    if(prob[i]>=0.5):\n",
    "        ans=1\n",
    "    else:\n",
    "        ans=0\n",
    "    lstmlabels.append(ans)\n",
    "\n",
    "# Accuracy : one can use classification_report from sklearn\n",
    "target_names=['negative','positive']\n",
    "print(classification_report(test_labels,lstmlabels,target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities\n",
      "\n",
      "[[0.9574223 ]\n",
      " [0.81979936]\n",
      " [0.5764051 ]]\n",
      "positive\n",
      "positive\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "# reviews on which we need to predict\n",
    "sentence = [\"The collections of the Philadelphia Museum of Art contain over 240,000 objects.\", \n",
    "            \"Daksh Varshney batchie ka kaat ta hai.\", \n",
    "            \"Massachusetts is far away from Rhode Island.\"]\n",
    "\n",
    "# convert to a sequence\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "\n",
    "# pad the sequence\n",
    "padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "\n",
    "# Get probabilities\n",
    "print(\"Probabilities\\n\")\n",
    "randomprob=model.predict(padded)\n",
    "print(randomprob)\n",
    "randomlabels=[]\n",
    "\n",
    "# Get labels based on probability 1 if p>= 0.5 else 0\n",
    "for i in range(3):\n",
    "    if(randomprob[i][0]>=0.75):\n",
    "        randomlabels.append(1)\n",
    "    else:\n",
    "        randomlabels.append(0)\n",
    "for i in range(len(randomlabels)):\n",
    "    if(randomlabels[i]==1):\n",
    "        print('positive')\n",
    "    else:\n",
    "        print('negative')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23300995598eec4bcf6bd89cf02d1c3675e8b2616661418dbbf5580aa901878d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
